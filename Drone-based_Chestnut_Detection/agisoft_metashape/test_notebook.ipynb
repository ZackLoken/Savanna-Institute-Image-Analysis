{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Processing Drone Imagery - Metashape Pro</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Metashape\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define input and output folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = \"S:/Zack/Imagery/Chestnut/Agisoft/2024/Wintergreen/AVNW/Raw Images/\"\n",
    "output_folder = \"S:/Zack/Imagery/Chestnut/Agisoft/2024/Wintergreen/AVNW/Outputs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Metashape project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Metashape.Document()\n",
    "doc.save(output_folder + \"project.psx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scan input folder for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(folder, types):\n",
    "    return [entry.path for entry in os.scandir(folder) if (entry.is_file() and os.path.splitext(entry.name)[1].upper() in types)]\n",
    "\n",
    "# load photos from image folder\n",
    "photos = find_files(image_folder, [\".JPG\", \".JPEG\", \".TIF\", \".TIFF\"])\n",
    "\n",
    "print(len(photos), \"photos found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load images from folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = doc.addChunk()\n",
    "chunk.addPhotos(photos)\n",
    "\n",
    "print(str(len(chunk.cameras)) + \" images loaded\")\n",
    "\n",
    "doc.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Align the loaded images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk.alignCameras(adaptive_fitting=False,\n",
    "                   min_image=2)\n",
    "doc.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate tie points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk.matchPhotos(downscale=0, \n",
    "                  keypoint_limit=60000, \n",
    "                  tiepoint_limit=0, \n",
    "                  generic_preselection=True, \n",
    "                  reference_preselection=True,\n",
    "                  filter_mask=True,\n",
    "                  mask_tiepoints=True,\n",
    "                  filter_stationary_points=True,\n",
    "                  keep_keypoints=False,\n",
    "                  reset_matches=False)\n",
    "doc.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Realign matched images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk.alignCameras(adaptive_fitting=False,\n",
    "                   min_image=2)\n",
    "doc.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimize cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk.optimizeCameras(tiepoint_covariance=True,\n",
    "                      fit_f=True,\n",
    "                      fit_cx=True,\n",
    "                      fit_cy=True,\n",
    "                      fit_b1=True,\n",
    "                      fit_b2=True,\n",
    "                      fit_k1=True,\n",
    "                      fit_k2=True,\n",
    "                      fit_k3=True,\n",
    "                      fit_k4=True,\n",
    "                      fit_p1=True,\n",
    "                      fit_p2=True,\n",
    "                      fit_corrections=True,\n",
    "                      adaptive_fitting=True)\n",
    "doc.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_selection_RU(RU_thrsh, num_tries=4, pct_del=10, thrsh_incr=1):\n",
    "    n=0\n",
    "    target_thrsh = 10\n",
    "    init_thrsh = RU_thrsh\n",
    "    points = chunk.tie_points.points\n",
    "    # compute number of starting points\n",
    "    points_start_num = len(points)\n",
    "    f = Metashape.TiePoints.Filter()\n",
    "    f.init(chunk, criterion = Metashape.TiePoints.Filter.ReconstructionUncertainty)\n",
    "    # select points that exceed initial threshold \n",
    "    f.selectPoints(init_thrsh)\n",
    "    # count number of points selected\n",
    "    nselected = len([True for point in points if point.valid is True and point.selected is True])\n",
    "    # check to see if any points were selected\n",
    "    pct_selected = (nselected / points_start_num)*100\n",
    "    # decrease the initial threshold if few points were selected\n",
    "    if pct_selected <= 1:\n",
    "        while True:\n",
    "            print(f\"Current threshold is {init_thrsh}. Adjusting downward by {thrsh_incr}\")\n",
    "            init_thrsh -= thrsh_incr\n",
    "            f.selectPoints(init_thrsh)\n",
    "            nselected = len([True for point in points if point.valid is True and point.selected is True])\n",
    "            ps = (nselected / points_start_num)*100\n",
    "            if ps >= 1:\n",
    "                break\n",
    "    # increase the initial threshold if too many points were selected\n",
    "    elif pct_selected > pct_del:\n",
    "        while True:\n",
    "            if pct_selected > pct_del:\n",
    "                init_thrsh += thrsh_incr\n",
    "                f.selectPoints(init_thrsh)\n",
    "                nselected = len([True for point in points if point.valid is True and point.selected is True])\n",
    "                ps = (nselected / points_start_num)*100\n",
    "                if ps < pct_del:\n",
    "                    break\n",
    "    print(f\"New adjusted initial threshold is {init_thrsh}. Beginning gradual selection.\")\n",
    "    # begin iterative gradual selection\n",
    "    n+=1\n",
    "    print(f\"Removing {nselected} points.\")\n",
    "    f.removePoints(init_thrsh)\n",
    "    init_thrsh -= thrsh_incr\n",
    "    chunk.optimizeCameras(adaptive_fitting=True,fit_f = True, fit_cx = True, fit_cy = True, fit_b1 = True, fit_b2 = True, fit_k1 = True, fit_k2 = True, fit_k3 = True, fit_k4 = True, fit_p1 = True, fit_p2 = True)\n",
    "    points = chunk.tie_points.points\n",
    "    npoints = len(points)\n",
    "    while True:\n",
    "        n+=1\n",
    "        if n>num_tries or init_thrsh<=target_thrsh or (100*((points_start_num-npoints)/points_start_num))>=50:\n",
    "            break\n",
    "        else:\n",
    "            points = chunk.tie_points.points\n",
    "            npoints = len(points)\n",
    "            f.selectPoints(init_thrsh)\n",
    "            nselected = len([True for point in points if point.valid is True and point.selected is True])\n",
    "            pct_selected = (nselected / npoints)*100\n",
    "            while True:\n",
    "                if pct_selected <= pct_del:\n",
    "                    init_thrsh -= thrsh_incr/5\n",
    "                    f.selectPoints(init_thrsh)\n",
    "                    nselected = len([True for point in points if point.valid is True and point.selected is True])\n",
    "                    pct_selected = (nselected/npoints)*100\n",
    "                else:\n",
    "                    break\n",
    "            f.selectPoints(init_thrsh)\n",
    "            nselected = len([True for point in points if point.valid is True and point.selected is True])\n",
    "            if nselected > 0:\n",
    "                print(f\"Removing {nselected} points at a threshold of {init_thrsh}\")\n",
    "                f.removePoints(init_thrsh)\n",
    "                init_thrsh -= thrsh_incr\n",
    "                chunk.optimizeCameras(adaptive_fitting=True,fit_f = True, fit_cx = True, fit_cy = True, fit_b1 = True, fit_b2 = True, fit_k1 = True, fit_k2 = True, fit_k3 = True, fit_k4 = True, fit_p1 = True, fit_p2 = True)\n",
    "                points = chunk.tie_points.points\n",
    "                npoints = len(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_selection_PA(PA_thrsh, num_tries=4, pct_del=10, thrsh_incr=1):\n",
    "    n=0\n",
    "    target_thrsh = 3\n",
    "    init_thrsh=PA_thrsh\n",
    "    points = chunk.tie_points.points\n",
    "    points_start_num = len(points)\n",
    "    npoints = len(points)\n",
    "    f = Metashape.TiePoints.Filter()\n",
    "    f.init(chunk, criterion = Metashape.TiePoints.Filter.ProjectionAccuracy)\n",
    "    f.selectPoints(init_thrsh)\n",
    "    # count number of points selected\n",
    "    nselected = len([True for point in points if point.valid is True and point.selected is True])\n",
    "    # check to see if any points were selected\n",
    "    pct_selected = (nselected / points_start_num)*100\n",
    "    if init_thrsh<=target_thrsh and pct_selected <= 50: # job done\n",
    "        print(f\"Now removing {nselected} points at a threshold of {init_thrsh}\")\n",
    "        f.removePoints(init_thrsh)\n",
    "        chunk.optimizeCameras(adaptive_fitting=True,fit_f = True, fit_cx = True, fit_cy = True, fit_b1 = True, fit_b2 = True, fit_k1 = True, fit_k2 = True, fit_k3 = True, fit_k4 = True, fit_p1 = True, fit_p2 = True)\n",
    "    else:\n",
    "        while True:\n",
    "            n+=1\n",
    "            if n>num_tries or init_thrsh<=target_thrsh or (100*((points_start_num-npoints)/points_start_num))>=50:\n",
    "                break\n",
    "            else:\n",
    "                points = chunk.tie_points.points\n",
    "                npoints = len(points)\n",
    "                f.selectPoints(init_thrsh)\n",
    "                nselected = len([True for point in points if point.valid is True and point.selected is True])\n",
    "                pct_selected = (nselected / npoints)*100\n",
    "                while True:\n",
    "                    if pct_selected <= pct_del:\n",
    "                        init_thrsh -= thrsh_incr/5\n",
    "                        f.selectPoints(init_thrsh)\n",
    "                        nselected = len([True for point in points if point.valid is True and point.selected is True])\n",
    "                        pct_selected = (nselected/npoints)*100\n",
    "                    else:\n",
    "                        break\n",
    "                f.selectPoints(init_thrsh)\n",
    "                nselected = len([True for point in points if point.valid is True and point.selected is True])\n",
    "                if nselected > 0:\n",
    "                    print(f\"Removing {nselected} points at a threshold of {init_thrsh}\")\n",
    "                    f.removePoints(init_thrsh)\n",
    "                    init_thrsh -= thrsh_incr\n",
    "                    chunk.optimizeCameras(adaptive_fitting=True,fit_f = True, fit_cx = True, fit_cy = True, fit_b1 = True, fit_b2 = True, fit_k1 = True, fit_k2 = True, fit_k3 = True, fit_k4 = True, fit_p1 = True, fit_p2 = True)\n",
    "                    points = chunk.tie_points.points\n",
    "                    npoints = len(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_selection_RE(RE_thrsh, num_tries=10, pct_del=10, thrsh_incr=0.1):\n",
    "    n=0\n",
    "    target_thrsh=0.3\n",
    "    init_thrsh=RE_thrsh\n",
    "    points = chunk.tie_points.points\n",
    "    points_start_num = len(points)\n",
    "    npoints = len(points)\n",
    "    f = Metashape.TiePoints.Filter()\n",
    "    f.init(chunk, criterion = Metashape.TiePoints.Filter.ReprojectionError)\n",
    "    f.selectPoints(init_thrsh)\n",
    "    # count number of points selected\n",
    "    nselected = len([True for point in points if point.valid is True and point.selected is True])\n",
    "    # check to see if any points were selected\n",
    "    pct_selected = (nselected / points_start_num)*100\n",
    "    if init_thrsh <= target_thrsh and pct_selected <= pct_del: # job done\n",
    "        print(f\"Now removing {nselected} points at a threshold of {init_thrsh}\")\n",
    "        f.removePoints(init_thrsh)\n",
    "        chunk.optimizeCameras(adaptive_fitting=True,fit_f = True, fit_cx = True, fit_cy = True, fit_b1 = True, fit_b2 = True, fit_k1 = True, fit_k2 = True, fit_k3 = True, fit_k4 = True, fit_p1 = True, fit_p2 = True)\n",
    "    else:\n",
    "        while True:\n",
    "            n+=1\n",
    "            if n>num_tries or init_thrsh<=target_thrsh or (100*((points_start_num-npoints)/points_start_num))>=pct_del:\n",
    "                break\n",
    "            else:\n",
    "                points = chunk.tie_points.points\n",
    "                npoints = len(points)\n",
    "                f.selectPoints(init_thrsh)\n",
    "                nselected = len([True for point in points if point.valid is True and point.selected is True])\n",
    "                pct_selected = (nselected / npoints)*100\n",
    "                while True:\n",
    "                    if pct_selected <= pct_del:\n",
    "                        init_thrsh -= thrsh_incr\n",
    "                        f.selectPoints(init_thrsh)\n",
    "                        nselected = len([True for point in points if point.valid is True and point.selected is True])\n",
    "                        pct_selected = (nselected/npoints)*100\n",
    "                    else:\n",
    "                        break\n",
    "                f.selectPoints(init_thrsh)\n",
    "                nselected = len([True for point in points if point.valid is True and point.selected is True])\n",
    "                if nselected > 0:\n",
    "                    print(f\"Removing {nselected} points at a threshold of {init_thrsh}\")\n",
    "                    f.removePoints(init_thrsh)\n",
    "                    init_thrsh -= thrsh_incr\n",
    "                    chunk.optimizeCameras(adaptive_fitting=True,fit_f = True, fit_cx = True, fit_cy = True, fit_b1 = True, fit_b2 = True, fit_k1 = True, fit_k2 = True, fit_k3 = True, fit_k4 = True, fit_p1 = True, fit_p2 = True)\n",
    "                    points = chunk.tie_points.points\n",
    "                    npoints = len(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter tie points before reconstruction\n",
    "RU_thrsh = 12\n",
    "PA_thrsh = 6\n",
    "RE_thrsh = 0.5\n",
    "\n",
    "# reconstruction uncertainty\n",
    "grad_selection_RU(RU_thrsh)\n",
    "doc.save()\n",
    "\n",
    "# projection accuracy\n",
    "print(f\"Now performing gradual selection using projection accuracy with a threshold of {PA_thrsh}\")\n",
    "grad_selection_PA(PA_thrsh)\n",
    "doc.save()\n",
    "\n",
    "# reprojection error\n",
    "print(f\"Now performing gradual selection using reprojection error with a threshold of {RE_thrsh}\")\n",
    "grad_selection_RE(RE_thrsh)\n",
    "doc.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build depth maps and point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build depth maps\n",
    "chunk.buildDepthMaps(downscale = 1, \n",
    "                     filter_mode = Metashape.MildFiltering,\n",
    "                     reuse_depth = True)\n",
    "doc.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open doc from output folder not read_only mode\n",
    "doc = Metashape.Document()\n",
    "doc.open(output_folder + \"project.psx\", read_only=False, ignore_lock=True)\n",
    "chunk = doc.chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter dense cloud\n",
    "point_cloud = doc.chunk.point_cloud\n",
    "point_cloud.setConfidenceFilter(0, 1)\n",
    "point_cloud.cropSelectedPoints()\n",
    "point_cloud.setConfidenceFilter(0, 255)\n",
    "point_cloud.compactPoints()\n",
    "doc.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check for transforms and apply if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_transform = chunk.transform.scale and chunk.transform.rotation and chunk.transform.translation\n",
    "\n",
    "if has_transform:\n",
    "    chunk.buildPointCloud(source_data=Metashape.DepthMaps, point_colors=True, point_confidence=True, keep_depth=True)\n",
    "    doc.save()\n",
    "\n",
    "    # filter dense cloud\n",
    "    point_cloud = doc.chunk.point_cloud\n",
    "    point_cloud.setConfidenceFilter(0, 1)\n",
    "    point_cloud.cropSelectedPoints()\n",
    "    point_cloud.setConfidenceFilter(0, 255)\n",
    "    point_cloud.compactPoints()\n",
    "    doc.save()\n",
    "\n",
    "    chunk.buildDem(source_data=Metashape.PointCloud)\n",
    "    doc.save()\n",
    "\n",
    "    chunk.buildOrthomosaic(surface_data=Metashape.Model)\n",
    "    doc.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export processing outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if chunk.point_cloud:\n",
    "    chunk.exportPointCloud(output_folder + 'point_cloud.las', source_data = Metashape.PointCloud)\n",
    "\n",
    "if chunk.elevation:\n",
    "    chunk.exportRaster(output_folder + 'dem.tif', source_data = Metashape.Elevation)\n",
    "\n",
    "if chunk.orthomosaic:\n",
    "    chunk.exportRaster(output_folder + 'orthomosaic.tif', source_data = Metashape.Orthomosaic)\n",
    "\n",
    "# export results\n",
    "chunk.exportReport(output_folder + 'report.pdf')\n",
    "\n",
    "print('Processing finished, results saved to ' + output_folder + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of tie points\n",
    "tie_points = chunk.point_cloud\n",
    "print(f\"Total tie points: {len(tie_points)}\")\n",
    "\n",
    "# Mean reprojection error\n",
    "if len(tie_points) > 0:\n",
    "    mean_reproj_error = sum(p.error for p in tie_points if p.valid) / len(tie_points)\n",
    "    print(f\"Mean reprojection error: {mean_reproj_error:.4f} pixels\")\n",
    "\n",
    "# Check how many cameras are aligned\n",
    "aligned_cameras = sum(1 for cam in chunk.cameras if cam.transform is not None)\n",
    "total_cameras = len(chunk.cameras)\n",
    "print(f\"Aligned Cameras: {aligned_cameras}/{total_cameras}\")\n",
    "\n",
    "# Print alignment success rate\n",
    "alignment_rate = (aligned_cameras / total_cameras) * 100\n",
    "print(f\"Alignment Success Rate: {alignment_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open project \n",
    "doc = Metashape.Document()\n",
    "doc.open(\"S:\\\\Zack\\\\Imagery\\\\Chestnut\\\\Agisoft\\\\2024\\\\Wintergreen\\\\AVNW\\\\Outputs_20250318_164146\\\\20250318_164146_project.psx\", read_only=False, ignore_lock=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = \"20250318_164146\"\n",
    "folder = \"S:\\\\Zack\\\\Imagery\\\\Chestnut\\\\Agisoft\\\\2024\\\\Wintergreen\\\\AVNW\\\\Outputs_20250318_164146\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during DEM/Model/Orthomosaic building for folder S:\\Zack\\Imagery\\Chestnut\\Agisoft\\2024\\Wintergreen\\AVNW\\Outputs_20250318_164146\\: cuCtxCreate failed: CUDA_ERROR_UNKNOWN (999) at line 196\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    has_transform = (doc.chunk.transform.scale and doc.chunk.transform.rotation and doc.chunk.transform.translation)\n",
    "    if has_transform:\n",
    "        # # Build point cloud from depth maps\n",
    "        # chunk.buildPointCloud(source_data=Metashape.DataSource.DepthMapsData, \n",
    "        #                         point_colors=True, \n",
    "        #                         point_confidence=True, \n",
    "        #                         keep_depth=True)\n",
    "        # print(\"Point cloud finished building.\")\n",
    "        # chunk.point_cloud.setConfidenceFilter(0, 1)\n",
    "        # chunk.point_cloud.cropSelectedPoints()\n",
    "        # chunk.point_cloud.setConfidenceFilter(0, 255)\n",
    "        # chunk.point_cloud.compactPoints()\n",
    "        # print(\"Point cloud filtered.\")\n",
    "        # chunk.point_cloud.classifyGroundPoints(cell_size=0.25)\n",
    "        # print(\"Point cloud ground points classified.\")\n",
    "        # doc.save()\n",
    "        \n",
    "        # # Build DEM, Model, and Orthomosaic\n",
    "        # doc.chunk.buildDem(source_data=Metashape.DataSource.PointCloudData,\n",
    "        #                 interpolation=Metashape.Interpolation.EnabledInterpolation)\n",
    "        # print(\"DEM finished building.\")\n",
    "        # doc.save()\n",
    "        \n",
    "        doc.chunk.buildModel(source_data=Metashape.DataSource.DepthMapsData, \n",
    "                            surface_type=Metashape.SurfaceType.Arbitrary, \n",
    "                            interpolation=Metashape.Interpolation.EnabledInterpolation,\n",
    "                            face_count=Metashape.FaceCount.HighFaceCount,\n",
    "                            split_in_blocks=True,\n",
    "                            workitem_size_cameras=15,\n",
    "                            max_workgroup_size=75)\n",
    "        print(\"Mesh finished building.\")\n",
    "        doc.save()\n",
    "        \n",
    "        doc.chunk.buildOrthomosaic(surface_data=Metashape.DataSource.ModelData, \n",
    "                                blending_mode=Metashape.BlendingMode.MosaicBlending,\n",
    "                                ghosting_filter=True,\n",
    "                                fill_holes=True,\n",
    "                                cull_faces=False,\n",
    "                                refine_seamlines=True)\n",
    "        print(\"Orthomosaic finished building.\")\n",
    "        doc.save()\n",
    "except Exception as e:\n",
    "    print(f\"Error during DEM/Model/Orthomosaic building for folder {folder}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CoordinateSystem 'WGS 84 (EPSG::4326)'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgs84 = Metashape.CoordinateSystem(\"EPSG::4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Export results\n",
    "    if doc.chunk.point_cloud:\n",
    "        pc_file = os.path.join(folder, f\"{current_time}_point_cloud.las\")\n",
    "        doc.chunk.exportPointCloud(pc_file, source_data=Metashape.DataSource.PointCloudData, crs=wgs84)\n",
    "        print(\"Point cloud exported.\")\n",
    "\n",
    "    if doc.chunk.elevation:\n",
    "        dem_file = os.path.join(folder, f\"{current_time}_dem.tif\")\n",
    "        doc.chunk.exportRaster(dem_file, source_data=Metashape.DataSource.ElevationData)\n",
    "        print(\"DEM exported.\")\n",
    "\n",
    "    if doc.chunk.orthomosaic:\n",
    "        ortho_file = os.path.join(folder, f\"{current_time}_orthomosaic.tif\")\n",
    "        doc.chunk.exportRaster(ortho_file, source_data=Metashape.DataSource.OrthomosaicData)\n",
    "        print(\"Orthomosaic exported.\")\n",
    "\n",
    "    report_file = os.path.join(folder, f\"{current_time}_report.pdf\")\n",
    "    doc.chunk.exportReport(report_file)\n",
    "    print(\"Report exported.\")\n",
    "\n",
    "    print(f\"Processing finished for folder {folder}; results saved to {folder}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during export for folder {folder}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metashape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
